---
title: "Week 7: Building a ML End-to-End Project"
output: html_notebook
---

## Lab1: Regression Machine Learning Case Study Project

# Import packages

```{r}
library(caret)
library(corrplot)
library(mlbench)
library(kernlab)

data(BostonHousing)

```

# Validation Dataset

```{r}

set.seed(7)

# Create a list of 80% of the rows in the original dataset we can use for training

validationIndex <- createDataPartition(BostonHousing$medv, p = .80, list = FALSE)

# Select 20% of the data for validation

validation <- BostonHousing[-validationIndex, ]

# Use the remaining 80% of data for training and testing the models

dataset <- BostonHousing[validationIndex, ]

```


# Analyse data

```{r}

# Dimensions of dataset

dim(dataset)

# list types for each attribute

sapply(dataset, class)

# Take a peek at the first 20 rows of the dataset

head(dataset, n=20)

# summarize attribute distributions

summary(dataset)

```


# Converting types and Coreelation

```{r}
# Uni lab had errors (corrected)

# convert chas to a numeric attribute

dataset$chas <- as.numeric(as.character(dataset$chas))

# correlation between all the numeric attributes

cor(dataset[, sapply(dataset, is.numeric)])

```


# Unimodal data visualizations

```{r}

# histograms each attribute

par(mfrow = c(2,7))

for (i in 1:13) {
  hist(dataset[, i], main = names(dataset)[i])
  
}

```

```{r}

# density plot for each attribute

par(mfrow = c(2,7))
for (i in 1:13) {
  plot(density(dataset[,i]), main = names(dataset)[i])
  
}

```

```{r}

# Boxplots for each attribute

par(mfrow = c(2,7))
for (i in 1:13) {
  boxplot(dataset[,i], main = names(dataset)[i])
  
}
```

```{r}

# scatterplot matrix

pairs(dataset[,1:13])

```

```{r}

# correlation plot

correlations <- cor(dataset[,1:13])

corrplot(correlations, method = "circle")

```


# Cross validation

Cross-validation is a technique used in statistics and machine learning to evaluate the performance of a model. It helps ensure that the model generalizes well to new, unseen data. Here’s a simple breakdown:

Purpose: The main goal of cross-validation is to test the model’s ability to predict new data that wasn’t used during training. This helps in identifying issues like overfitting, where the model performs well on training data but poorly on new data.

Process: The data is divided into multiple subsets or “folds”. The model is trained on some of these folds and tested on the remaining ones. This process is repeated several times, each time using a different fold as the test set. The results are then averaged to get a more accurate estimate of the model’s performance.

Types:
K-Fold Cross-Validation: The data is divided into ‘k’ subsets. The model is trained on ‘k-1’ subsets and tested on the remaining one. This is repeated ‘k’ times.

Leave-One-Out Cross-Validation (LOOCV): Each data point is used once as a test set while the rest are used for training. This is repeated for each data point.

Holdout Validation: The data is split into two sets, one for training and one for testing. This is simpler but less reliable than k-fold cross-validation

```{r}
# Prepare the test harness for evaluating algorithms

# Run algorithms using 10-fold cross validation

trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

metric <- "RMSE"

```


# Estimate accuracy of machine learning algorithms

```{r}

#LM
set.seed(7)
fit.lm <- train(medv~., data = dataset, method = 'lm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)


# GLM
set.seed(7)
fit.glm <- train(medv~., data = dataset, method = 'glm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data = dataset, method = 'glmnet', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# SVM
set.seed(7)
fit.svm <- train(medv~., data = dataset, method = 'svmRadial', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# CART
set.seed(7)
grid <- expand.grid(.cp= c(0, 0.5, 0.1))
fit.cart <- train(medv~., data = dataset, method = 'rpart', metric = metric, tuneGrid = grid, preProc = c("center", "scale"), trControl = trainControl)

# KNN
set.seed(7)
fit.knn <- train(medv~., data = dataset, method = 'knn', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

```


# Collect resampled statistics from models and summarise results

```{r}

# compare algorithms

results <- resamples(list(LM= fit.lm, GLM = fit.glm, GLMNET = fit.glmnet, SVM = fit.svm, CART = fit.cart, KNN = fit.knn))

summary(results)

dotplot(results)

```

It looks like SVM has the lowest RMSE, followed by the other non-linear algorithms CART and KNN. The linear regression algorithms all appear to be in the same ballpark with slightly worse error.

We can also see that SVM and the other non-linear algorithms have the best fit for the data in their r2 measures.

Did centering and scaling make a difference to the algorithms other than KNN? Doubtful, and best to hold the data constant at this stage. Perhaps the worse performance of the linear algorithms has something to do with the highly correlated attributes. Let's look at that in the next section.

# Evaluate Algorithms: Feature Selection

We have a theory that the correlated attributes are reducing the accuracy of the linear algorithms tried in the base line spot-check in the previous step. In this step we will remove the highly correlated attributes and see what effect that has on the evaluation metrics. We can find and remove the highly correlated attributes using the findCorrelation() function from the caret package.

# Remove highly correlated attributes from the dataset

```{r}

# find attributes that are highly correlated

set.seed(7)

cutoff <- 0.70

correlations <- cor(dataset[,1:13])

highlyCorrelated <- findCorrelation(correlations, cutoff = cutoff)

for (value in highlyCorrelated) {
  print(names(dataset)[value])
  
}

```

Now let's try the same 6 algorithms from our base line experiment

# Estimate accuracy of models on modified dataset.

```{r}

# Run algorithms using 10-fold cross validation

trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

metric <- "RMSE"

#LM
set.seed(7)
fit.lm <- train(medv~., data = dataset, method = 'lm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)


# GLM
set.seed(7)
fit.glm <- train(medv~., data = dataset, method = 'glm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data = dataset, method = 'glmnet', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# SVM
set.seed(7)
fit.svm <- train(medv~., data = dataset, method = 'svmRadial', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# CART
set.seed(7)
grid <- expand.grid(.cp= c(0, 0.5, 0.1))
fit.cart <- train(medv~., data = dataset, method = 'rpart', metric = metric, tuneGrid = grid, preProc = c("center", "scale"), trControl = trainControl)

# KNN
set.seed(7)
fit.knn <- train(medv~., data = dataset, method = 'knn', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

```


```{r}

# compare algorithms

feature_results <- resamples(list(LM= fit.lm, GLM = fit.glm, GLMNET = fit.glmnet, SVM = fit.svm, CART = fit.cart, KNN = fit.knn))

summary(feature_results)

dotplot(feature_results)

```

Comparing the results, we can see that this has made the RMSE worse for the linear and non-linear algorithms. The correlated attributes we removed are contributing to the accuracy of the models.


# Evaluated algorithms: Box-Cox Transform

We know that some of the attributes have a skew and others perhaps have an exponential distribution. One option would be to explore squaring and log transforms respectively. Another approach would be to use a power transform and let it figure out the amount to correct each attribute. One example is the Box-Cox power transform. Let's try using this transform to rescale the original dat and evaluate the effect on the same 6 algorithms. We will also leave in the centering and scaling for the benefit of the instance-based method.

# Estimate acuuracy of algorithms on transform dataset

```{r}

# Run algorithms using 10-fold cross validation

trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

metric <- "RMSE"

#LM
set.seed(7)
fit.lm <- train(medv~., data = dataset, method = 'lm', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)


# GLM
set.seed(7)
fit.glm <- train(medv~., data = dataset, method = 'glm', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)

# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data = dataset, method = 'glmnet', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)

# SVM
set.seed(7)
fit.svm <- train(medv~., data = dataset, method = 'svmRadial', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)

# CART
set.seed(7)
grid <- expand.grid(.cp= c(0, 0.5, 0.1))
fit.cart <- train(medv~., data = dataset, method = 'rpart', metric = metric, tuneGrid = grid, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)

# KNN
set.seed(7)
fit.knn <- train(medv~., data = dataset, method = 'knn', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)


# compare algorithms

transformResults <- resamples(list(LM= fit.lm, GLM = fit.glm, GLMNET = fit.glmnet, SVM = fit.svm, CART = fit.cart, KNN = fit.knn))

summary(transformResults)

dotplot(transformResults)

```

We can see that this indeed decreased the RMSE and increased the R2 on all except the CART algorithm.The RMSE of SVM dropped to an average of 3.703.


# Improve model with tuning

We can improve the accuracy of the well performing algorithms by tuning their parameters. In this
section we will look at tuning the parameters of SVM with a Radial Basis Function (RBF). with more
time it might be worth exploring tuning of the parameters for CART and KNN. It might also be
worth exploring other kernels for SVM besides the RBF. Let's look at the default parameters
already adopted.


# Display estimated accuracy of a model

```{r}

print(fit.svm)

```

Let's design a grid search around a C value of 1. We might see a small trend of decreasing RMSE
with increasing C, so let’s try all integer C values between 1 and 10. Another parameter that caret
lets us tune is the sigma parameter. This is a smoothing parameter. Good sigma values are often
start around 0.1, so we will try numbers before and after.


# Tune the parameters of a model.

```{r}
# tune SVM sigma and C parameters

trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
set.seed(7)
grid <- expand.grid(.sigma=c(0.025, 0.05, 0.1, 0.15), .C=seq(1, 10,
by=1))
fit.svm <- train(medv~., data=dataset, method="svmRadial", metric=metric,
tuneGrid=grid,
preProc=c("BoxCox"), trControl=trainControl)
print(fit.svm)
plot(fit.svm)

```

We can see that the sigma values flatten out with larger C cost constraints. It looks like we might
do well with a sigma of 0.1 and a C of 10. This gives us a respectable RMSE of 3.084181.

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.1 and C = 10

If we wanted to take this further, we could try even more fine tuning with more grid searches. We
could also explore trying to tune other parameters of the underlying ksvm() function. Finally and
as already mentioned, we could perform some grid searches on the other non-linear regression
methods


# Ensemble Methods


We can try some ensemble methods on the problem and see if we can get a further decrease in
our RMSE. In this section we will look at some boosting and bagging techniques for decision trees.
Additional approaches you could look into would be blending the predictions of multiple well
performing models together, called stacking. Let's take a look at the following ensemble methods:

Random Forest, bagging (RF).
Gradient Boosting Machines boosting (GBM).
Cubist, boosting (CUBIST).

# Estimate accuracy of ensemble methods

```{r}

# try ensemble

trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"

# Random Forest
set.seed(7)
fit.rf <- train(medv~., data=dataset, method="rf", metric=metric,
preProc=c("BoxCox"), trControl=trainControl)

# Stochastic Gradient Boosting
set.seed(7)
fit.gbm <- train(medv~., data=dataset, method="gbm", metric=metric,
preProc=c("BoxCox"), trControl=trainControl, verbose=FALSE)

# Cubist
set.seed(7)
fit.cubist <- train(medv~., data=dataset, method="cubist", metric=metric,
preProc=c("BoxCox"), trControl=trainControl)

# Compare algorithms
ensembleResults <- resamples(list(RF=fit.rf, GBM=fit.gbm,
CUBIST=fit.cubist))
summary(ensembleResults)
dotplot(ensembleResults)

```

We can see that Cubist was the most accurate.

Let's dive deeper into Cubist and see if we can tune it further and get more skill out of it. Cubist
has two parameters that are tuneable with caret: committees which is the number of boosting
operations and neighbours which is used during prediction and is the number of instances used to
correct the rule based prediction (although the documentation is perhaps a little ambiguous on
this). For more information about Cubist see the help on the function ?cubist. Let's first look at the
default tuning parameter used by caret that resulted in our accurate model.

# Summarize accuracy of a model.

```{r}

# look at parameters used for Cubist
print(fit.cubist)

```

We can see that the best RMSE was achieved with committees = 20 and neighbors = 5

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were committees = 20 and neighbors = 5.

Let's use a grid search to tune around those values. We'll try all committees between 15 and 25
and spot-check a neighbours value above and below 5.

# Tune the parameters of a model.

```{r}

# Tune the Cubist algorithm
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
set.seed(7)
grid <- expand.grid(.committees=seq(15, 25, by=1), .neighbors=c(3, 5, 7))
tune.cubist <- train(medv~., data=dataset, method="cubist",
metric=metric,
preProc=c("BoxCox"), tuneGrid=grid, trControl=trainControl)
print(tune.cubist)
plot(tune.cubist)

```

We can see that we have achieved a more accurate model again with an RMSE of 3.028333 using
committees = 18 and neighbors = 3

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were committees = 18 and neighbors =
3.

With more time we could tune the Cubist algorithm further. Also, with results like this, it also
suggests it might be worth investigating whether we can get more out of the GBM or other
boosting implementations.


# Finalise the Model

It looks like that cubist results in our most accurate model. Let's finalise it by creating a new
standalone Cubist model with the parameters above trained using the whole dataset. We must
also use the Box-Cox power transform.

# Prepare the data transform and finalize the model

```{r}

# prepare the data transform using training data

library(Cubist)

set.seed(7)

x <- dataset[,1:13]
y <- dataset[,14]
preprocessParams <- preProcess(x, method=c("BoxCox"))
transX <- predict(preprocessParams, x)
# train the final model
finalModel <- cubist(x=transX, y=y, committees=18)
summary(finalModel)

```

We can now use this model to evaluate our held-out validation dataset. Again, we must prepare
the input data using the same Box-Cox transform.

# Make predictions using the final model

```{r}

# transform the validation dataset

set.seed(7)
valX <- validation[,1:13]
trans_valX <- predict(preprocessParams, valX)
valY <- validation[,14]

# use final model to make predictions on the validation dataset

predictions <- predict(finalModel, newdata=trans_valX, neighbors=3)

# calculate RMSE

rmse <- RMSE(predictions, valY)
r2 <- R2(predictions, valY)
print(rmse)

```


## Lab2: Binary Classification Machine Learning Project

# Import packages

```{r}

library(dplyr)
library(tidyr)
library(corrgram)
library(ggplot2)
library(ggthemes)
library(cluster)
library(caret)
library(ggplot2)

# Insert dataset into R

med <- read.csv("cancer_data.csv", sep = ",", header = TRUE)

# Discard the id column as it will not be used in any of the analysis below

med <- med[, 2:12]

# change the name of the first column to diagnosis

colnames(med)[1] <- "diagnosis"

```


# EDA

Exploratory Data Analysis and Visualisations
Before using the machine learning algorithms for classification task, it is essential to have an
overview of the dataset. Below there is a box-plot of each predictor against the target variable
(tumour). The log value of the predictors used instead of the actual values, for a better view of the
plot.

```{r}

# Create a long version of the dataset
med2 <- gather(med, "feature", "n", 2:11)
ggplot(med2)+
geom_boxplot(aes(diagnosis, log(n)))+
facet_wrap(~feature, scales = "free")+
labs(title = "Box-plot of all predictors(log scaled) per tumor type",
subtitle = "tumor can be either malignant -M- or benign -B-")+
theme_fivethirtyeight()+
theme(axis.title = element_text()) +
ylab("Predictor's log value") +
xlab('')

```

It seems that for most predictors the malignant level of tumour type has higher values than the
benign level. Now let’s see if the predictors are correlated. Below there is a scatter-plot matrix of
all predictors

