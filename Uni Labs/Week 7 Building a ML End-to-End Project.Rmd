---
title: "Week 7: Building a ML End-to-End Project"
output: html_notebook
---


# Import packages

```{r}
library(caret)
library(corrplot)
library(mlbench)

data(BostonHousing)

```

# Validation Dataset

```{r}

set.seed(7)

# Create a list of 80% of the rows in the original dataset we can use for training

validationIndex <- createDataPartition(BostonHousing$medv, p = .80, list = FALSE)

# Select 20% of the data for validation

validation <- BostonHousing[-validationIndex, ]

# Use the remaining 80% of data for training and testing the models

dataset <- BostonHousing[validationIndex, ]

```


# Analyse data

```{r}

# Dimensions of dataset

dim(dataset)

# list types for each attribute

sapply(dataset, class)

# Take a peek at the first 20 rows of the dataset

head(dataset, n=20)

# summarize attribute distributions

summary(dataset)

```


# Converting types and Coreelation

```{r}
# Uni lab had errors (corrected)

# convert chas to a numeric attribute

dataset$chas <- as.numeric(as.character(dataset$chas))

# correlation between all the numeric attributes

cor(dataset[, sapply(dataset, is.numeric)])

```


# Unimodal data visualizations

```{r}

# histograms each attribute

par(mfrow = c(2,7))

for (i in 1:13) {
  hist(dataset[, i], main = names(dataset)[i])
  
}

```

```{r}

# density plot for each attribute

par(mfrow = c(2,7))
for (i in 1:13) {
  plot(density(dataset[,i]), main = names(dataset)[i])
  
}

```

```{r}

# Boxplots for each attribute

par(mfrow = c(2,7))
for (i in 1:13) {
  boxplot(dataset[,i], main = names(dataset)[i])
  
}
```

```{r}

# scatterplot matrix

pairs(dataset[,1:13])

```

```{r}

# correlation plot

correlations <- cor(dataset[,1:13])

corrplot(correlations, method = "circle")

```


# Cross validation

Cross-validation is a technique used in statistics and machine learning to evaluate the performance of a model. It helps ensure that the model generalizes well to new, unseen data. Here’s a simple breakdown:

Purpose: The main goal of cross-validation is to test the model’s ability to predict new data that wasn’t used during training. This helps in identifying issues like overfitting, where the model performs well on training data but poorly on new data.

Process: The data is divided into multiple subsets or “folds”. The model is trained on some of these folds and tested on the remaining ones. This process is repeated several times, each time using a different fold as the test set. The results are then averaged to get a more accurate estimate of the model’s performance.

Types:
K-Fold Cross-Validation: The data is divided into ‘k’ subsets. The model is trained on ‘k-1’ subsets and tested on the remaining one. This is repeated ‘k’ times.

Leave-One-Out Cross-Validation (LOOCV): Each data point is used once as a test set while the rest are used for training. This is repeated for each data point.

Holdout Validation: The data is split into two sets, one for training and one for testing. This is simpler but less reliable than k-fold cross-validation

```{r}
# Prepare the test harness for evaluating algorithms

# Run algorithms using 10-fold cross validation

trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

metric <- "RMSE"

```


# Estimate accuracy of machine learning algorithms

```{r}

#LM
set.seed(7)
fit.lm <- train(medv~., data = dataset, method = 'lm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)


# GLM
set.seed(7)
fit.glm <- train(medv~., data = dataset, method = 'glm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data = dataset, method = 'glmnet', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# SVM
set.seed(7)
fit.svm <- train(medv~., data = dataset, method = 'svmRadial', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# CART
set.seed(7)
grid <- expand.grid(.cp= c(0, 0.5, 0.1))
fit.cart <- train(medv~., data = dataset, method = 'rpart', metric = metric, tuneGrid = grid, preProc = c("center", "scale"), trControl = trainControl)

# KNN
set.seed(7)
fit.knn <- train(medv~., data = dataset, method = 'knn', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

```

