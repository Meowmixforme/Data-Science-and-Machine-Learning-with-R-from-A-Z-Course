---
title: "Week 7: Building a ML End-to-End Project"
output: html_notebook
---


# Import packages

```{r}
library(caret)
library(corrplot)
library(mlbench)
library(kernlab)

data(BostonHousing)

```

# Validation Dataset

```{r}

set.seed(7)

# Create a list of 80% of the rows in the original dataset we can use for training

validationIndex <- createDataPartition(BostonHousing$medv, p = .80, list = FALSE)

# Select 20% of the data for validation

validation <- BostonHousing[-validationIndex, ]

# Use the remaining 80% of data for training and testing the models

dataset <- BostonHousing[validationIndex, ]

```


# Analyse data

```{r}

# Dimensions of dataset

dim(dataset)

# list types for each attribute

sapply(dataset, class)

# Take a peek at the first 20 rows of the dataset

head(dataset, n=20)

# summarize attribute distributions

summary(dataset)

```


# Converting types and Coreelation

```{r}
# Uni lab had errors (corrected)

# convert chas to a numeric attribute

dataset$chas <- as.numeric(as.character(dataset$chas))

# correlation between all the numeric attributes

cor(dataset[, sapply(dataset, is.numeric)])

```


# Unimodal data visualizations

```{r}

# histograms each attribute

par(mfrow = c(2,7))

for (i in 1:13) {
  hist(dataset[, i], main = names(dataset)[i])
  
}

```

```{r}

# density plot for each attribute

par(mfrow = c(2,7))
for (i in 1:13) {
  plot(density(dataset[,i]), main = names(dataset)[i])
  
}

```

```{r}

# Boxplots for each attribute

par(mfrow = c(2,7))
for (i in 1:13) {
  boxplot(dataset[,i], main = names(dataset)[i])
  
}
```

```{r}

# scatterplot matrix

pairs(dataset[,1:13])

```

```{r}

# correlation plot

correlations <- cor(dataset[,1:13])

corrplot(correlations, method = "circle")

```


# Cross validation

Cross-validation is a technique used in statistics and machine learning to evaluate the performance of a model. It helps ensure that the model generalizes well to new, unseen data. Here’s a simple breakdown:

Purpose: The main goal of cross-validation is to test the model’s ability to predict new data that wasn’t used during training. This helps in identifying issues like overfitting, where the model performs well on training data but poorly on new data.

Process: The data is divided into multiple subsets or “folds”. The model is trained on some of these folds and tested on the remaining ones. This process is repeated several times, each time using a different fold as the test set. The results are then averaged to get a more accurate estimate of the model’s performance.

Types:
K-Fold Cross-Validation: The data is divided into ‘k’ subsets. The model is trained on ‘k-1’ subsets and tested on the remaining one. This is repeated ‘k’ times.

Leave-One-Out Cross-Validation (LOOCV): Each data point is used once as a test set while the rest are used for training. This is repeated for each data point.

Holdout Validation: The data is split into two sets, one for training and one for testing. This is simpler but less reliable than k-fold cross-validation

```{r}
# Prepare the test harness for evaluating algorithms

# Run algorithms using 10-fold cross validation

trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

metric <- "RMSE"

```


# Estimate accuracy of machine learning algorithms

```{r}

#LM
set.seed(7)
fit.lm <- train(medv~., data = dataset, method = 'lm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)


# GLM
set.seed(7)
fit.glm <- train(medv~., data = dataset, method = 'glm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data = dataset, method = 'glmnet', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# SVM
set.seed(7)
fit.svm <- train(medv~., data = dataset, method = 'svmRadial', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# CART
set.seed(7)
grid <- expand.grid(.cp= c(0, 0.5, 0.1))
fit.cart <- train(medv~., data = dataset, method = 'rpart', metric = metric, tuneGrid = grid, preProc = c("center", "scale"), trControl = trainControl)

# KNN
set.seed(7)
fit.knn <- train(medv~., data = dataset, method = 'knn', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

```


# Collect resampled statistics from models and summarise results

```{r}

# compare algorithms

results <- resamples(list(LM= fit.lm, GLM = fit.glm, GLMNET = fit.glmnet, SVM = fit.svm, CART = fit.cart, KNN = fit.knn))

summary(results)

dotplot(results)

```

It looks like SVM has the lowest RMSE, followed by the other non-linear algorithms CART and KNN. The linear regression algorithms all appear to be in the same ballpark with slightly worse error.

We can also see that SVM and the other non-linear algorithms have the best fit for the data in their r2 measures.

Did centering and scaling make a difference to the algorithms other than KNN? Doubtful, and best to hold the data constant at this stage. Perhaps the worse performance of the linear algorithms has something to do with the highly correlated attributes. Let's look at that in the next section.

# Evaluate Algorithms: Feature Selection

We have a theory that the correlated attributes are reducing the accuracy of the linear algorithms tried in the base line spot-check in the previous step. In this step we will remove the highly correlated attributes and see what effect that has on the evaluation metrics. We can find and remove the highly correlated attributes using the findCorrelation() function from the caret package.

# Remove highly correlated attributes from the dataset

```{r}

# find attributes that are highly correlated

set.seed(7)

cutoff <- 0.70

correlations <- cor(dataset[,1:13])

highlyCorrelated <- findCorrelation(correlations, cutoff = cutoff)

for (value in highlyCorrelated) {
  print(names(dataset)[value])
  
}

```

Now let's try the same 6 algorithms from our base line experiment

# Estimate accuracy of models on modified dataset.

```{r}

# Run algorithms using 10-fold cross validation

trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

metric <- "RMSE"

#LM
set.seed(7)
fit.lm <- train(medv~., data = dataset, method = 'lm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)


# GLM
set.seed(7)
fit.glm <- train(medv~., data = dataset, method = 'glm', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data = dataset, method = 'glmnet', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# SVM
set.seed(7)
fit.svm <- train(medv~., data = dataset, method = 'svmRadial', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

# CART
set.seed(7)
grid <- expand.grid(.cp= c(0, 0.5, 0.1))
fit.cart <- train(medv~., data = dataset, method = 'rpart', metric = metric, tuneGrid = grid, preProc = c("center", "scale"), trControl = trainControl)

# KNN
set.seed(7)
fit.knn <- train(medv~., data = dataset, method = 'knn', metric = metric, preProc = c("center", "scale"), trControl = trainControl)

```


```{r}

# compare algorithms

feature_results <- resamples(list(LM= fit.lm, GLM = fit.glm, GLMNET = fit.glmnet, SVM = fit.svm, CART = fit.cart, KNN = fit.knn))

summary(feature_results)

dotplot(feature_results)

```

Comparing the results, we can see that this has made the RMSE worse for the linear and non-linear algorithms. The correlated attributes we removed are contributing to the accuracy of the models.


# Evaluated algorithms: Box-Cox Transform

We know that some of the attributes have a skew and others perhaps have an exponential distribution. One option would be to explore squaring and log transforms respectively. Another approach would be to use a power transform and let it figure out the amount to correct each attribute. One example is the Box-Cox power transform. Let's try using this transform to rescale the original dat and evaluate the effect on the same 6 algorithms. We will also leave in the centering and scaling for the benefit of the instance-based method.

# Estimate acuuracy of algorithms on transform dataset

```{r}

# Run algorithms using 10-fold cross validation

trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

metric <- "RMSE"

#LM
set.seed(7)
fit.lm <- train(medv~., data = dataset, method = 'lm', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)


# GLM
set.seed(7)
fit.glm <- train(medv~., data = dataset, method = 'glm', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)

# GLMNET
set.seed(7)
fit.glmnet <- train(medv~., data = dataset, method = 'glmnet', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)

# SVM
set.seed(7)
fit.svm <- train(medv~., data = dataset, method = 'svmRadial', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)

# CART
set.seed(7)
grid <- expand.grid(.cp= c(0, 0.5, 0.1))
fit.cart <- train(medv~., data = dataset, method = 'rpart', metric = metric, tuneGrid = grid, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)

# KNN
set.seed(7)
fit.knn <- train(medv~., data = dataset, method = 'knn', metric = metric, preProc = c("center", "scale", "BoxCox"), trControl = trainControl)


# compare algorithms

transformResults <- resamples(list(LM= fit.lm, GLM = fit.glm, GLMNET = fit.glmnet, SVM = fit.svm, CART = fit.cart, KNN = fit.knn))

summary(transformResults)

dotplot(transformResults)

```

We can see that this indeed decreased the RMSE and increased the R2 on all except the CART algorithm.The RMSE of SVM dropped to an average of 3.703.


# Improve model with tuning

We can improve the accuracy of the well performing algorithms by tuning their parameters. In this
section we will look at tuning the parameters of SVM with a Radial Basis Function (RBF). with more
time it might be worth exploring tuning of the parameters for CART and KNN. It might also be
worth exploring other kernels for SVM besides the RBF. Let's look at the default parameters
already adopted.


# Display estimated accuracy of a model

```{r}

print(fit.svm)

```

Let's design a grid search around a C value of 1. We might see a small trend of decreasing RMSE
with increasing C, so let’s try all integer C values between 1 and 10. Another parameter that caret
lets us tune is the sigma parameter. This is a smoothing parameter. Good sigma values are often
start around 0.1, so we will try numbers before and after.


# Tune the parameters of a model.

```{r}
# tune SVM sigma and C parameters

trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
set.seed(7)
grid <- expand.grid(.sigma=c(0.025, 0.05, 0.1, 0.15), .C=seq(1, 10,
by=1))
fit.svm <- train(medv~., data=dataset, method="svmRadial", metric=metric,
tuneGrid=grid,
preProc=c("BoxCox"), trControl=trainControl)
print(fit.svm)
plot(fit.svm)

```

We can see that the sigma values flatten out with larger C cost constraints. It looks like we might
do well with a sigma of 0.1 and a C of 10. This gives us a respectable RMSE of 3.084181.

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.1 and C = 10

If we wanted to take this further, we could try even more fine tuning with more grid searches. We
could also explore trying to tune other parameters of the underlying ksvm() function. Finally and
as already mentioned, we could perform some grid searches on the other non-linear regression
methods


# Ensemble Methods


We can try some ensemble methods on the problem and see if we can get a further decrease in
our RMSE. In this section we will look at some boosting and bagging techniques for decision trees.
Additional approaches you could look into would be blending the predictions of multiple well
performing models together, called stacking. Let's take a look at the following ensemble methods:

Random Forest, bagging (RF).
Gradient Boosting Machines boosting (GBM).
Cubist, boosting (CUBIST).

# Estimate accuracy of ensemble methods

```{r}

# try ensemble


```

