---
title: "Week 4: Classification"
output: html_notebook
---

# import the data

```{r}

gc <- read.csv("german_credit.csv")



```

# check the data

```{r}
head(gc)
```

# understanding data structure

```{r}
str(gc)
```

# Feature / Attribute Selection

# The variable 'Credibility' is out target variable i.e. this variable will determine whether the bank manager will approve a loan based on the 7 Attributes.

```{r}
gc.subset <- gc[c('Creditability', 'Age..years.', 'Sex...Marital.Status', 'Occupation', 'Account.Balance', 'Credit.Amount', 'Length.of.current.employment', 'Purpose')]

head(gc.subset)
```

# Data normalization to avoid biasness as the value scale of 'Credit.Amount' is in thousands whereas other attributes value are in 2 digit or 1 digit.

```{r}

normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x))) # creating a normalize function for easy conversion
}

gc.subset.n <- as.data.frame(lapply(gc.subset[,2:8], normalize)) # lapply creates a list that is why it is converted to a dataframe and it applies the defined function (which is 'normalize') to all the list values which is here column 2 to 8 as first column is target/response.

head(gc.subset.n)
```

# Creating Training and Test data set

```{r}
set.seed(123) # To get the same random sample

dat.d <- sample(1:nrow(gc.subset.n), size = nrow(gc.subset.n) * 0.7, replace = FALSE) # Random selection of 70% data.

train.gc <- gc.subset[dat.d,] # 70% training data
test.gc <- gc.subset[-dat.d,] # remaining 30% test data

# Now creating separate dataframe for 'Credibility' feature which is our target.

train.gc_labels <- gc.subset[dat.d, 1]
test.gc_labels <- gc.subset[-dat.d, 1]

```

# Training the model

```{r}
library(class)

NROW(train.gc_labels) # to find number of observations

# To identify optimum value of k, generally square root of total no of observations (700) which is 26.45 is taken, so will try with 26, 27 and then will check for optimal value of k.

knn.26 <- knn(train = train.gc, test = test.gc, cl = train.gc_labels, k = 26)

knn.27 <- knn(train = train.gc, test = test.gc, cl = train.gc_labels, k = 27)
  
  
```

# Evaluate model performance

```{r}

ACC.26 <- 100 * sum(test.gc_labels == knn.26) / NROW(test.gc_labels) # for knn = 26

ACC.27 <- 100 * sum(test.gc_labels == knn.27) / NROW(test.gc_labels) # for knn = 27

ACC.26 

ACC.27 # At 69 is most accurate (Uni Lab papers were worng .. cough.. cough...)

table(knn.26, test.gc_labels) # to check prediction against actual value in tabular form


table(knn.27, test.gc_labels)

```
Accuracy can also be calculated using the 'caret' package and 'confusion matrix' function

```{r}
# For knn.26

library(caret)

test.gc_labels <- as.factor(test.gc_labels)
confusionMatrix(knn.26, test.gc_labels)

## Confusion matrix and statistics
```

```{r}
# For knn.27

test.gc_labels <- as.factor(test.gc_labels)
confusionMatrix(knn.27, test.gc_labels)

```

# Improve the model

For k-NN algorithm, the tuning parameters are 'k' value and number of 'features/attributes' selection.

Optimum 'k' value can be found using 'maximum % accuracy' graph but 'feature selection' can be done through understanding of features in k-NN algorithm.

```{r}

i=1 # declaration for infinite for loop
k.optm = 1 # declaration for infinite for loop

for (i in 1:128) {
  knn.mod <- knn(train = train.gc, test = test.gc, cl = train.gc_labels, k = i)
  
  k.optm[i] <- 100 * sum(test.gc_labels == knn.mod) / NROW(test.gc_labels)
  k = i
  cat(k, '=', k.optm[i], '\n') # to print % accuracy
  
}

plot(k.optm, type = "b", xlab = "K-Value", ylab = "Accuracy level") # to plot % accuracy to k-value

```

At k=17, maximum accuracy is achieved which is 69%, after that, the accuracy seems stable. It is worse to class a customer as good when it is bad, than it is to class a customer bad when it is good.

Further accuracy can be increased by optimizing feature selections and repeating the above mentioned algorithm.

## Lab 2

```{r}

df <- data(iris) ## load data

head(iris) ## see the structure

```

```{r}

## Generate a random number that is 90% of the total number of rows in the dataset.

ran <- sample(1:nrow(iris),0.9 * nrow(iris))

## The normalization function is created

nor <-function(x) {
  (x -min(x) / (max(x) - min(x)))
}

# Run the normalization on first 4 columns of dataset because they are the predictors

iris_norm <- as.data.frame(lapply(iris[c(1,2,3,4)], nor))

```



```{r}

# Extract training set

iris_train <- iris_norm[ran,]

# Extract testing set

iris_test <- iris_norm[-ran,]

iris_target_category <- iris[ran, 5] # extract 5th column of train dataset because it will be used as 'cl' argument in k-nn function

iris_test_category <- iris[-ran, 5] # extract 5th column of test dataset to measure the accuracy

library(class)

pr <- knn(iris_train, iris_test, cl = iris_target_category, k =13)

## Create confusion matrix

tab <- table(pr, iris_test_category)

# this function divides the correct predictions by total number of predictions that tell us how accurate the model is.

accuracy <- function(x) {
  sum(diag(x) / sum(rowSums(x))) * 100
}

accuracy(tab)

```
## Lab 3

```{r}

# Because diamonds dataset is in ggplot2 package

library(ggplot2)
data(diamonds)

dia <- data.frame(diamonds) # Store as a data frame

ran <- sample(1:nrow(dia), 0.9 * nrow(dia)) # create a random number equal 90% of total number of rows

#the normalization function is created
nor <- function(x) { 
  (x -min(x)) / (max(x) -min(x))
}

dia_nor <- as.data.frame(lapply(dia[,c(1,5,6,7,8,9,10)], nor))

# training dataset extracted
dia_train <- dia_nor[ran, ]

# test dataset extracted
dia_test <- dia_nor[-ran,]

dia_target <- as.factor(dia[ran,2])

test_target <- as.factor(dia[-ran, 2])

# run knn functions

library(class)

pr <- knn(dia_train, dia_test, cl = dia_target, k = 20)

# create the confusion matrix
tb <- table(pr, test_target)

# Check accuracy

accuracy <- function(x) {
  sum(diag(x) / (sum(rowSums(x)))) * 100
}

accuracy(tb)

```
# Detecting prostate cancer

```{r}

# import the data

library(readr)

prc <- read.csv("Prostate_Cancer.csv", stringsAsFactors = FALSE) # imports data and converts every chat to a factor wherever it makes sense

str(prc)

```
# id is not useful and so should be removed

```{r}
prc <- prc[-1] # removed the first variable (column) from the dataset (id)

```

The dataset contains patients who have been diagnosed with either Malignant (M) or Benign (B) cancer

```{r}

table(prc$diagnosis_result) # it helps us to get number of patients with B & M

```

In case we wish to rename B as "Benign" and M as "Malignant" and see the results in percentage form

```{r}
# WARNING: Run either this or previous cell but not both as it will wipe the data

prc$diagnosis_result <- factor(prc$diagnosis_result, levels = c("B", "M"), labels = c("Benign", "Malignant"))

round(prop.table(table(prc$diagnosis_result)) * 100, digits = 1)

```

# Normalizing numeric data

```{r}

normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

```

# Apply normalization on the dataset

```{r}

prc_n <- as.data.frame(lapply(prc[2:9], normalize))

```

# Check data has been normalized by viewing radius

```{r}

summary(prc_n$radius)

```

# Creating train and test data

```{r}
prc_train <- prc_n[1:65,]

prc_test <- prc_n[66:100,]

prc_train_labels <- prc[1:65, 1] # Include diagnosis_factor to label the data

prc_test_labels <- prc[66:100, 1]

```

# Calssify data with k-NN 

```{r}

library(class)

prc_test_pred <- knn(train = prc_train, test = prc_test, cl = prc_train_labels, k =10) # k = square root of num of observations

```

# Evaluate model performance

```{r}

library(gmodels)

CrossTable(x = prc_test_labels, y = prc_test_pred, prop.chisq = FALSE)

```
Improve accuracy by repeating steps 3 & 4 and changing the k value + or - 10 while keeping FN;s as low as possible.
