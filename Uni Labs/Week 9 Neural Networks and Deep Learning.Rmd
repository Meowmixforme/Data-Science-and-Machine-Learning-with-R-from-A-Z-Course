---
title: "Week 9: Neural Networks and Deep Learning"
output: html_notebook
---



```{r}

library(mlbench)

data("BreastCancer")

# Clean off rows with missing data

BreastCancer = BreastCancer[which(complete.cases(BreastCancer)==TRUE),]

head(BreastCancer)

```


In this dataset, there are close to 700 samples of tissue taken in biopsies. For each
biopsy, nine different characteristics are recorded such as cell thickness, cell size, cell
shape. etc. The column names in the dataset are as follows:

```{r}

names(BreastCancer)

```

The last column in the dataset is “Class” which is either bening or malignant. The goal of
the analysis is to construct a model that learns to decide whether the tumor is malignant
or not.

The first package in R that we will explore is the deepnet package. Details may be
accessed at https://cran.r-project.org/web/packages/deepnet/index.html. We apply the
package to the cancer dataset as follows. First, we create the dependent variable, and
also the feature set of independent variables.

```{r}

y = as.matrix(BreastCancer[,11])
y[which(y=="benign")] = 0
y[which(y=="malignant")] = 1
y = as.numeric(y)
x = as.numeric(as.matrix(BreastCancer[,2:10]))
x = matrix(as.numeric(x),ncol=9)

```

We then use the function nn.train from the deepnet package to model the neural
network. As can be seen in the program code below, we have 5 nodes in the single
hidden layer.

```{r}

library(deepnet)
nn <- nn.train(x, y, hidden = c(5))
yy = nn.predict(nn, x)
print(head(yy))

```

We take the output of the network and convert it into classes, such that class “0” is
benign and class “1” is malignant. We then construct the “confusion matrix” to see how
well the model does in-sample. The table function here creates the confusion matrix,
which is a tabulation of how many observations that were benign and malignant were
correctly classified. This is a handy way of assessing how successful a machine
learning model is at classification.

```{r}

yhat = matrix(0,length(yy),1)
yhat[which(yy > mean(yy))] = 1
yhat[which(yy <= mean(yy))] = 0
cm = table(y,yhat)
print(cm)

```

We can see that the diagonal of the confusion matrix contains most of the entries,
thereby suggesting that the neural net does a very good job of classification. 
The accuracy may be computed easily as the number of diagonal entries in the confusion
matrix divided by the total count of values in the matrix.

```{r}

print(sum(diag(cm))/sum(cm))

```

Now that we have seen the model work you can delve into the function nn.train in some
more detail to examine the options it allows. The reference manual for the package is
available at https://cran.r-project.org/web/packages/deepnet/deepnet.pdf.

For comparison, we try the neuralnet package. The commands are mostly the same.
The function in the package is also called neuralnet.

```{r}

library(neuralnet)

df = data.frame(cbind(x,y))
nn = neuralnet(y~V1+V2+V3+V4+V5+V6+V7+V8+V9,data=df,hidden = 5)
yy = nn$net.result[[1]]
yhat = matrix(0,length(y),1)
yhat[which(yy > mean(yy))] = 1
yhat[which(yy <= mean(yy))] = 0
print(table(y,yhat))

```

This package also performs very well on this dataset. Details about the package and its
various functions are available at:

https://cran.r-project.org/web/packages/neuralnet/index.html

This package has an interesting function that allows plotting the neural network. Use the
function plot() and pass the output object to it, in this case nn.

The good folks at h2o, see http://www.h2o.ai/, have developed a Java-based version of
R, in which they also provide a deep learning network application.

H2O is open source, in-memory, distributed, fast, and provides a scalable machine
learning and predictive analytics platform for building machine learning models on big
data. H2O’s core code is written in Java. Inside H2O, a Distributed Key/Value store is
used to access and reference data, models, objects, etc., across all nodes and
machines. The algorithms are implemented in a Map/Reduce framework and utilizes
multi-threading. The data is read in parallel and is distributed across the cluster and
stored in memory in a columnar format in a compressed way. Therefore, even on a
single machine, the deep learning algorithm in H2O will exploit all cores of the CPU in
parallel.

Here we start up a server using all cores of the machine, and then use the H2O
package’s deep learning toolkit to fit a model.

```{r}

library(h2o)

localH2O = h2o.init(ip="localhost", port = 54321,
startH2O = TRUE, nthreads=-1)
train <- h2o.importFile("BreastCancer.csv")
test <- h2o.importFile("BreastCancer.csv")
y = names(train)[11]
x = names(train)[1:10]

train[,y] = as.factor(train[,y])
test[,y] = as.factor(train[,y])
model = h2o.deeplearning(x=x,
y=y,
training_frame=train,
validation_frame=test,
distribution = "multinomial",
activation = "RectifierWithDropout",
hidden = c(10,10,10,10),
input_dropout_ratio = 0.2,
l1 = 1e-5,
epochs = 50)
print(model)

```

The h2o deep learning package does very well. The error rate may be seen from the
confusion matrix to be very low. We also note that H2O may be used to run analyses
other than deep learning in R as well, as many other functions are provided, using
almost identical syntax to R. See the documentation at H2O for more
details: https://docs.h2o.ai/h2o/latest-stable/index.html


As a second case, we use the MNIST dataset, replicating an example from the H2O
deep learning manual. This character (numerical digits) recognition example is a classic
one in machine learning. First read in the data.

```{r}
library(h2o)
localH2O = h2o.init(ip="localhost", port = 54321,
startH2O = TRUE)
## Import MNIST CSV as H2O
train <- h2o.importFile
("https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/
train.csv.gz")
test <- h2o.importFile
("https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/
test.csv.gz")
print(dim(train))
print(dim(test))
```

As we see there are 70,000 observations in the dataset with each example containing
all the 784 pixels in each image, defining the character. This suggests a very large input
dataset. Now, we have a much larger parameter space that needs to be fit by the deep
learning net. We use a three hidden layer model, with each hidden layer having 10
nodes.

```{r}
y <- "C785"
x <- setdiff(names(train), y)
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])
# Train a Deep Learning model and validate on a test set
model <- h2o.deeplearning(x = x,
y = y,
training_frame = train,
validation_frame = test,
distribution = "multinomial",
activation = "RectifierWithDropout",
hidden = c(10,10,10),
input_dropout_ratio = 0.2,
l1 = 1e-5,
epochs = 20)
print(model)
```

The mean error is much higher here, around a third. It looks like the highest error arises
from the DLN mistaking the number “8” for the number “1”. It also seems to confuse the
number “3” for the number “5”. However, it appears to do best in identifying the numbers
“3” and “7”.
We repeat the model with a deeper net with more nodes to see if accuracy increases.

```{r}
y <- "C785"
x <- setdiff(names(train), y)
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])
# Train a Deep Learning model and validate on a test set
model <- h2o.deeplearning(x = x,
y = y,
training_frame = train,
validation_frame = test,
distribution = "multinomial",
activation = "RectifierWithDropout",
hidden = c(50,50,50,50,50),
input_dropout_ratio = 0.2,
l1 = 1e-5,
epochs = 20)
print(model)
```

In fact, now the error rate is greatly reduced. It is useful to assess whether the
improvement comes from more nodes in each layer or more hidden layers.

```{r}
y <- "C785"
x <- setdiff(names(train), y)
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])
# Train a Deep Learning model and validate on a test set
model <- h2o.deeplearning(x = x,
y = y,
training_frame = train,
validation_frame = test,
distribution = "multinomial",
activation = "RectifierWithDropout",
hidden = c(100,100,100),
input_dropout_ratio = 0.2,
l1 = 1e-5,
epochs = 20)
print(model)
```

The error rate is now extremely low, so the number of nodes per hidden layer seems to
matter more. However, we do need to note that this is more art than science, and we
should make sure that we try various different DLNs before settling on the final one for
our application.

There are two packages available for the front end of TensorFlow. In this section we will
use keras. In R the usage is slightly different, and the reader may prefer one versus the
other. Technically, there is no difference. The main difference is in the way we write
code for the two different alternatives.

```{r}
# Install the tensorflow R package from GitHub (need to install devtools first)
devtools::install_github("rstudio/tensorflow")

# Install Python via reticulate
reticulate::install_python()

library(tensorflow)
install_tensorflow(envname = "r-tensorflow")

library(magrittr)
library(keras)

model <- keras_model_sequential()
n_units = 100
tf_train <- read.csv("..PATH/BreastCancer.csv")
tf_test <- read.csv("..PATH/BreastCancer.csv")
X_train = as.matrix(tf_train[,2:10])
X_test = as.matrix(tf_test[,2:10])
y_train = as.matrix(tf_train[,11])
y_test = as.matrix(tf_test[,11])
idx = which(y_train=="benign"); y_train[idx]=0; y_train[-idx]=1;
y_train=as.integer(y_train)
idx = which(y_test=="benign"); y_test[idx]=0; y_test[-idx]=1;

# Next, we define the deep learning model.

n_units = 100
model %>%
layer_dense(units = n_units,
activation = 'relu',
input_shape = dim(X_train)[2]) %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = n_units, activation = 'relu') %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = n_units, activation = 'relu') %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = 10, activation = 'softmax')

# Now, compile the model.

model %>% compile(
loss = 'sparse_categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)

# Finally, fit the model. We will run just 5 epochs.

model %>% fit(
X_train, y_train,
epochs = 5, batch_size = 32, verbose = 1,
validation_split = 0.1
)

```

